{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.path import Path\n",
    "from tracker import Tracker\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below is originally created in the lesson at Udacity self-driving car nano degree.\n",
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Calculate directional gradient\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "\n",
    "    # Return the result\n",
    "    return binary_output\n",
    "\n",
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "\n",
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Calculate gradient direction\n",
    "    # Grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Calculate the x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Take the absolute value of the gradient direction, \n",
    "    # apply a threshold, and create a binary image result\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    binary_output =  np.zeros_like(absgraddir)\n",
    "    binary_output[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "\n",
    "# Define a function that thresholds the S-channel of HLS\n",
    "# Use exclusive lower bound (>) and inclusive upper (<=)\n",
    "def hls_select(img, thresh=(0, 255)):\n",
    "    # 1) Convert to HLS color space\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    # 2) Apply a threshold to the S channel\n",
    "    S = hls[:,:,2]\n",
    "    binary_output = np.zeros_like(S)\n",
    "    binary_output[(S > thresh[0]) & (S <= thresh[1])] = 1\n",
    "    # 3) Return a binary image of threshold result\n",
    "    return binary_output\n",
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved objpoints and imgpoints \n",
    "dist_pickle = pickle.load(open(\"camera_cal/calibration_pickle.p\", \"rb\"))\n",
    "mtx = dist_pickle[\"mtx\"]\n",
    "dist = dist_pickle[\"dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of calibration images\n",
    "images = glob.glob('test_images/test*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Distortion-corrected image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through the list\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    # undistort the image\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    dst = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "    # Visualize undistortion\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    ax2.imshow(dst)\n",
    "    ax2.set_title('Undistorted Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    plt.savefig('output_images/undistorted_test' + str(idx+1) + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Binary image results after gradient and color transformation with the  thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through the list\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    # undistort the image\n",
    "    undistort = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    undistort = cv2.cvtColor(undistort, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(undistort, orient='x', sobel_kernel=ksize, thresh=(20, 255))\n",
    "    grady = abs_sobel_thresh(undistort, orient='y', sobel_kernel=ksize, thresh=(30, 255))\n",
    "    #mag_binary = mag_thresh(undistort, sobel_kernel=ksize, mag_thresh=(30, 100))\n",
    "    #dir_binary = dir_threshold(undistort, sobel_kernel=ksize, thresh=(0.7, 1.3)) #, thresh=(0, np.pi/2))\n",
    "    c_binary = hls_select(undistort, thresh=(180, 255))\n",
    "\n",
    "    # Pre-process image template\n",
    "    preprocessImage = np.zeros_like(c_binary)\n",
    "    #preprocessImage[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1)) | (c_binary == 1)] = 1\n",
    "    preprocessImage[((gradx == 1) & (grady == 1)) | (c_binary == 1)] = 1\n",
    "\n",
    "    # Visualize undistortion\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    ax2.imshow(preprocessImage, cmap='gray')\n",
    "    ax2.set_title('Binary Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    plt.savefig('output_images/binary_test' + str(idx+1) + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3-1. Perspective transformation (Parameter Tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make a list of straight line test images\n",
    "images = glob.glob('test_images/straight_lines*.jpg')\n",
    "\n",
    "# Step through the list\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # undistort the image\n",
    "    undistort = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(undistort, orient='x', sobel_kernel=ksize, thresh=(20, 255))\n",
    "    grady = abs_sobel_thresh(undistort, orient='y', sobel_kernel=ksize, thresh=(30, 255))\n",
    "    #mag_binary = mag_thresh(undistort, sobel_kernel=ksize, mag_thresh=(30, 100))\n",
    "    #dir_binary = dir_threshold(undistort, sobel_kernel=ksize, thresh=(0.7, 1.3)) #, thresh=(0, np.pi/2))\n",
    "    c_binary = hls_select(undistort, thresh=(180, 255))\n",
    "\n",
    "    # Pre-process image template\n",
    "    preprocessImage = np.zeros_like(c_binary)\n",
    "    #preprocessImage[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1)) | (c_binary == 1)] = 1\n",
    "    preprocessImage[((gradx == 1) & (grady == 1)) | (c_binary == 1)] = 1\n",
    "\n",
    "    \n",
    "    # Perspective transformation\n",
    "    # paramters \n",
    "    upper_width_pct = .11     # Upper trapezoid width percentage\n",
    "    bottom_offset_pct = .04 # Bottom offset percentage\n",
    "    bottom_width_pct = .69  # Bottom trapezoid width percentage\n",
    "    height_pct = .32                  # Trapezoid height percentage\n",
    "    # Grab the image shape\n",
    "    img_size = (preprocessImage.shape[1], preprocessImage.shape[0])\n",
    "    offset = img_size[0] * .18 # offset for destination points\n",
    "\n",
    "    src_upper_left = (img_size[0] * (.5 - upper_width_pct / 2), img_size[1] * (1 - height_pct - bottom_offset_pct))\n",
    "    src_upper_right = (img_size[0] * (.5 + upper_width_pct / 2), img_size[1] * (1 - height_pct - bottom_offset_pct))\n",
    "    src_bottom_right = (img_size[0] * (.5 + bottom_width_pct / 2), img_size[1] * (1 - bottom_offset_pct))\n",
    "    src_bottom_left = (img_size[0] * (.5 - bottom_width_pct / 2), img_size[1] * (1 - bottom_offset_pct))\n",
    "    dst_upper_left = (offset, 0)\n",
    "    dst_upper_right = (img_size[0]-offset, 0)\n",
    "    dst_bottom_right = (img_size[0]-offset, img_size[1])\n",
    "    dst_bottom_left = (offset, img_size[1])\n",
    "    # For source points I'm grabbing the four trapezoid corners\n",
    "    src = np.float32([src_upper_left, src_upper_right, src_bottom_right, src_bottom_left])\n",
    "    # Destination points \n",
    "    dst = np.float32([dst_upper_left, dst_upper_right, dst_bottom_right, dst_bottom_left])\n",
    "    # cv2.getPerspectiveTransform() is used to get M, the transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # The inverse transformation from the destination to original source points\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # cv2.warpPerspective() is used to warp the original image to a top-down view\n",
    "    warped = cv2.warpPerspective(preprocessImage, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Visualize undistortion\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(undistort)\n",
    "    verts = [src_upper_left, src_upper_right, src_bottom_right, src_bottom_left, src_upper_left]\n",
    "    codes = [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n",
    "    patch = patches.PathPatch(Path(verts, codes), facecolor='none', edgecolor='red', lw=2)\n",
    "    ax1.add_patch(patch)\n",
    "    ax1.set_title('Undistorted Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    ax2.imshow(warped, cmap='gray')\n",
    "    ax2.set_title('Perspective Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    plt.savefig('output_images/perspective_straight_lines' + str(idx+1) + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3-2. Perspective transformation (Example Application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make a list of calibration images\n",
    "images = glob.glob('test_images/test*.jpg')\n",
    "\n",
    "# Step through the list\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # undistort the image\n",
    "    undistort = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(undistort, orient='x', sobel_kernel=ksize, thresh=(20, 255))\n",
    "    grady = abs_sobel_thresh(undistort, orient='y', sobel_kernel=ksize, thresh=(30, 255))\n",
    "    #mag_binary = mag_thresh(undistort, sobel_kernel=ksize, mag_thresh=(30, 100))\n",
    "    #dir_binary = dir_threshold(undistort, sobel_kernel=ksize, thresh=(0.7, 1.3)) #, thresh=(0, np.pi/2))\n",
    "    c_binary = hls_select(undistort, thresh=(170, 220))\n",
    "\n",
    "    # Pre-process image template\n",
    "    preprocessImage = np.zeros_like(c_binary)\n",
    "    #preprocessImage[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1)) | (c_binary == 1)] = 1\n",
    "    preprocessImage[((gradx == 1) & (grady == 1)) | (c_binary == 1)] = 1\n",
    "\n",
    "    \n",
    "    # Perspective transformation\n",
    "    # paramters \n",
    "    upper_width_pct = .11     # Upper trapezoid width percentage\n",
    "    bottom_offset_pct = .04 # Bottom offset percentage\n",
    "    bottom_width_pct = .69  # Bottom trapezoid width percentage\n",
    "    height_pct = .32                  # Trapezoid height percentage\n",
    "    # Grab the image shape\n",
    "    img_size = (preprocessImage.shape[1], preprocessImage.shape[0])\n",
    "    offset = img_size[0] * .15 # offset for destination points\n",
    "\n",
    "    src_upper_left = (img_size[0] * (.5 - upper_width_pct / 2), img_size[1] * (1 - height_pct - bottom_offset_pct))\n",
    "    src_upper_right = (img_size[0] * (.5 + upper_width_pct / 2), img_size[1] * (1 - height_pct - bottom_offset_pct))\n",
    "    src_bottom_right = (img_size[0] * (.5 + bottom_width_pct / 2), img_size[1] * (1 - bottom_offset_pct))\n",
    "    src_bottom_left = (img_size[0] * (.5 - bottom_width_pct / 2), img_size[1] * (1 - bottom_offset_pct))\n",
    "    dst_upper_left = (offset, 0)\n",
    "    dst_upper_right = (img_size[0]-offset, 0)\n",
    "    dst_bottom_right = (img_size[0]-offset, img_size[1])\n",
    "    dst_bottom_left = (offset, img_size[1])\n",
    "    # For source points I'm grabbing the four trapezoid corners\n",
    "    src = np.float32([src_upper_left, src_upper_right, src_bottom_right, src_bottom_left])\n",
    "    # Destination points \n",
    "    dst = np.float32([dst_upper_left, dst_upper_right, dst_bottom_right, dst_bottom_left])\n",
    "    # cv2.getPerspectiveTransform() is used to get M, the transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # The inverse transformation from the destination to original source points\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # cv2.warpPerspective() is used to warp the original image to a top-down view\n",
    "    warped = cv2.warpPerspective(preprocessImage, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Visualize undistortion\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(undistort)\n",
    "    verts = [src_upper_left, src_upper_right, src_bottom_right, src_bottom_left, src_upper_left]\n",
    "    codes = [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n",
    "    patch = patches.PathPatch(Path(verts, codes), facecolor='none', edgecolor='red', lw=2)\n",
    "    ax1.add_patch(patch)\n",
    "    ax1.set_title('Undistorted Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    ax2.imshow(warped, cmap='gray')\n",
    "    ax2.set_title('Perspective Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    plt.savefig('output_images/perspective_test' + str(idx+1) + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Perspective transformation with tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make a list of calibration images\n",
    "images = glob.glob('test_images/test*.jpg')\n",
    "\n",
    "# Step through the list\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # undistort the image\n",
    "    undistort = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(undistort, orient='x', sobel_kernel=ksize, thresh=(20, 255))\n",
    "    grady = abs_sobel_thresh(undistort, orient='y', sobel_kernel=ksize, thresh=(30, 255))\n",
    "    #mag_binary = mag_thresh(undistort, sobel_kernel=ksize, mag_thresh=(30, 100))\n",
    "    #dir_binary = dir_threshold(undistort, sobel_kernel=ksize, thresh=(0.7, 1.3)) #, thresh=(0, np.pi/2))\n",
    "    c_binary = hls_select(undistort, thresh=(170, 220))\n",
    "\n",
    "    # Pre-process image template\n",
    "    preprocessImage = np.zeros_like(c_binary)\n",
    "    #preprocessImage[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1)) | (c_binary == 1)] = 1\n",
    "    preprocessImage[((gradx == 1) & (grady == 1)) | (c_binary == 1)] = 255\n",
    "\n",
    "    \n",
    "    # Perspective transformation\n",
    "    # paramters \n",
    "    upper_width_pct = .11     # Upper trapezoid width percentage\n",
    "    bottom_offset_pct = .04 # Bottom offset percentage\n",
    "    bottom_width_pct = .69  # Bottom trapezoid width percentage\n",
    "    height_pct = .32                  # Trapezoid height percentage\n",
    "    # Grab the image shape\n",
    "    img_size = (preprocessImage.shape[1], preprocessImage.shape[0])\n",
    "    offset = img_size[0] * .15 # offset for destination points\n",
    "\n",
    "    src_upper_left = (img_size[0] * (.5 - upper_width_pct / 2), img_size[1] * (1 - height_pct - bottom_offset_pct))\n",
    "    src_upper_right = (img_size[0] * (.5 + upper_width_pct / 2), img_size[1] * (1 - height_pct - bottom_offset_pct))\n",
    "    src_bottom_right = (img_size[0] * (.5 + bottom_width_pct / 2), img_size[1] * (1 - bottom_offset_pct))\n",
    "    src_bottom_left = (img_size[0] * (.5 - bottom_width_pct / 2), img_size[1] * (1 - bottom_offset_pct))\n",
    "    dst_upper_left = (offset, 0)\n",
    "    dst_upper_right = (img_size[0]-offset, 0)\n",
    "    dst_bottom_right = (img_size[0]-offset, img_size[1])\n",
    "    dst_bottom_left = (offset, img_size[1])\n",
    "    # For source points I'm grabbing the four trapezoid corners\n",
    "    src = np.float32([src_upper_left, src_upper_right, src_bottom_right, src_bottom_left])\n",
    "    # Destination points \n",
    "    dst = np.float32([dst_upper_left, dst_upper_right, dst_bottom_right, dst_bottom_left])\n",
    "    # cv2.getPerspectiveTransform() is used to get M, the transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # The inverse transformation from the destination to original source points\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # cv2.warpPerspective() is used to warp the original image to a top-down view\n",
    "    warped = cv2.warpPerspective(preprocessImage, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    window_width = 25\n",
    "    window_height = 80\n",
    "    \n",
    "    # Set up the overall class to do all the tracking\n",
    "    curve_centers = Tracker(mywindow_width=window_width, mywindow_height=window_height, mymargin= 25, \n",
    "                            my_ym=10/720, my_xm=4/384, mysmooth_factor=15)\n",
    "    \n",
    "    window_centroids = curve_centers.find_window_centroids(warped)\n",
    "\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "\n",
    "        # Points used to find the left and right lanes\n",
    "        leftx = []\n",
    "        rightx = []\n",
    "\n",
    "        # Go through each level and draw the windows \n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Add center value found in frame to the list of lane points per left and right\n",
    "            leftx.append(window_centroids[level][0])\n",
    "            rightx.append(window_centroids[level][1])\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width, window_height, warped, window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width, window_height, warped, window_centroids[level][1],level)\n",
    "           # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "        # Draw the results\n",
    "        template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channle \n",
    "        template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "        warpage = np.array(cv2.merge((warped,warped,warped)),np.uint8) # making the original road pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "\n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "    \n",
    "    # Fit the lane boundaries to the left and right center poisitons found\n",
    "    yvals = range(0, warped.shape[0])\n",
    "    \n",
    "    # Adopt values from mid point of each windows to fit line\n",
    "    res_yvals = np.arange(warped.shape[0] - (window_height/2), 0, -window_height)\n",
    "    \n",
    "    # Fit a second order polynomial to pixel positions in each lane line\n",
    "    left_fit = np.polyfit(res_yvals, leftx, 2)\n",
    "    left_fitx = left_fit[0]*np.array(yvals)*np.array(yvals) + left_fit[1]*np.array(yvals) + left_fit[2]\n",
    "    left_fitx = np.array(left_fitx, np.int32)\n",
    "    right_fit = np.polyfit(res_yvals, rightx, 2)\n",
    "    right_fitx = right_fit[0]*np.array(yvals)*np.array(yvals) + right_fit[1]*np.array(yvals) + right_fit[2]\n",
    "    right_fitx = np.array(right_fitx, np.int32)\n",
    "\n",
    "    # Creating boundary for output lane to be used in cv2.fillPoly() function. \n",
    "    # Left downward edge first and then right upward edge, and concatenate them.\n",
    "    window_width = 10\n",
    "    left_lane = \\\n",
    "            np.array(list(zip(np.concatenate((left_fitx - window_width/2, left_fitx[::-1] + window_width/2), axis=0),\n",
    "                                         np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "    right_lane = \\\n",
    "            np.array(list(zip(np.concatenate((right_fitx - window_width/2, right_fitx[::-1] + window_width/2), axis=0),\n",
    "                                         np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "    inner_lane = \\\n",
    "        np.array(list(zip(np.concatenate((left_fitx + window_width / 2, right_fitx[::-1] - window_width / 2), axis=0),\n",
    "                          np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "\n",
    "    #road = np.zeros_like(output)\n",
    "    road = output\n",
    "    #road_bkg = np.zeros_like(img)\n",
    "    cv2.fillPoly(road, [left_lane], color=[255, 0, 0])\n",
    "    cv2.fillPoly(road, [right_lane], color=[0, 0, 255])\n",
    "    #cv2.fillPoly(road_bkg, [left_lane], color=[255, 255, 255])\n",
    "    #cv2.fillPoly(road_bkg, [right_lane], color=[255, 255, 255])\n",
    "    \n",
    "    output = road\n",
    "    \n",
    "    # Visualize undistortion\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(undistort)\n",
    "    verts = [src_upper_left, src_upper_right, src_bottom_right, src_bottom_left, src_upper_left]\n",
    "    codes = [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n",
    "    patch = patches.PathPatch(Path(verts, codes), facecolor='none', edgecolor='red', lw=2)\n",
    "    ax1.add_patch(patch)\n",
    "    ax1.set_title('Undistorted Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    ax2.imshow(output, cmap='gray')\n",
    "    ax2.set_title('Line fit Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    plt.savefig('output_images/line_fit_test' + str(idx+1) + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Undistorted Image with Lane Line Detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make a list of calibration images\n",
    "images = glob.glob('test_images/test*.jpg')\n",
    "\n",
    "# Step through the list\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # undistort the image\n",
    "    undistort = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "    # Apply each of the thresholding functions\n",
    "    gradx = abs_sobel_thresh(undistort, orient='x', sobel_kernel=ksize, thresh=(20, 255))\n",
    "    grady = abs_sobel_thresh(undistort, orient='y', sobel_kernel=ksize, thresh=(30, 255))\n",
    "    #mag_binary = mag_thresh(undistort, sobel_kernel=ksize, mag_thresh=(30, 100))\n",
    "    #dir_binary = dir_threshold(undistort, sobel_kernel=ksize, thresh=(0.7, 1.3)) #, thresh=(0, np.pi/2))\n",
    "    c_binary = hls_select(undistort, thresh=(170, 220))\n",
    "\n",
    "    # Pre-process image template\n",
    "    preprocessImage = np.zeros_like(c_binary)\n",
    "    #preprocessImage[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1)) | (c_binary == 1)] = 1\n",
    "    preprocessImage[((gradx == 1) & (grady == 1)) | (c_binary == 1)] = 255\n",
    "\n",
    "    \n",
    "    # Perspective transformation\n",
    "    # paramters \n",
    "    upper_width_pct = .11     # Upper trapezoid width percentage\n",
    "    bottom_offset_pct = .04 # Bottom offset percentage\n",
    "    bottom_width_pct = .69  # Bottom trapezoid width percentage\n",
    "    height_pct = .32                  # Trapezoid height percentage\n",
    "    # Grab the image shape\n",
    "    img_size = (preprocessImage.shape[1], preprocessImage.shape[0])\n",
    "    offset = img_size[0] * .18 # offset for destination points\n",
    "\n",
    "    src_upper_left = (img_size[0] * (.5 - upper_width_pct / 2), img_size[1] * (1 - height_pct - bottom_offset_pct))\n",
    "    src_upper_right = (img_size[0] * (.5 + upper_width_pct / 2), img_size[1] * (1 - height_pct - bottom_offset_pct))\n",
    "    src_bottom_right = (img_size[0] * (.5 + bottom_width_pct / 2), img_size[1] * (1 - bottom_offset_pct))\n",
    "    src_bottom_left = (img_size[0] * (.5 - bottom_width_pct / 2), img_size[1] * (1 - bottom_offset_pct))\n",
    "    dst_upper_left = (offset, 0)\n",
    "    dst_upper_right = (img_size[0]-offset, 0)\n",
    "    dst_bottom_right = (img_size[0]-offset, img_size[1])\n",
    "    dst_bottom_left = (offset, img_size[1])\n",
    "    # For source points I'm grabbing the four trapezoid corners\n",
    "    src = np.float32([src_upper_left, src_upper_right, src_bottom_right, src_bottom_left])\n",
    "    # Destination points \n",
    "    dst = np.float32([dst_upper_left, dst_upper_right, dst_bottom_right, dst_bottom_left])\n",
    "    # cv2.getPerspectiveTransform() is used to get M, the transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # The inverse transformation from the destination to original source points\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # cv2.warpPerspective() is used to warp the original image to a top-down view\n",
    "    warped = cv2.warpPerspective(preprocessImage, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    window_width = 25\n",
    "    window_height = 80\n",
    "    \n",
    "    # Set up the overall class to do all the tracking\n",
    "    curve_centers = Tracker(mywindow_width=window_width, mywindow_height=window_height, mymargin= 25, \n",
    "                            my_ym=10/720, my_xm=4/384, mysmooth_factor=15)\n",
    "    \n",
    "    window_centroids = curve_centers.find_window_centroids(warped)\n",
    "\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "\n",
    "        # Points used to find the left and right lanes\n",
    "        leftx = []\n",
    "        rightx = []\n",
    "\n",
    "        # Go through each level and draw the windows \n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Add center value found in frame to the list of lane points per left and right\n",
    "            leftx.append(window_centroids[level][0])\n",
    "            rightx.append(window_centroids[level][1])\n",
    "            # Window_mask is a function to draw window areas\n",
    "            #l_mask = window_mask(window_width, window_height, warped, window_centroids[level][0],level)\n",
    "            #r_mask = window_mask(window_width, window_height, warped, window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            #l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            #r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "        # Draw the results\n",
    "        #template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        #zero_channel = np.zeros_like(template) # create a zero color channle \n",
    "        #template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8) # making the original road pixels 3 color channels\n",
    "        #output = cv2.addWeighted(output, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "        \n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "    \n",
    "    # Fit the lane boundaries to the left and right center poisitons found\n",
    "    yvals = range(0, warped.shape[0])\n",
    "    \n",
    "    # Adopt values from mid point of each windows to fit line\n",
    "    res_yvals = np.arange(warped.shape[0] - (window_height/2), 0, -window_height)\n",
    "    \n",
    "    # Fit a second order polynomial to pixel positions in each lane line\n",
    "    left_fit = np.polyfit(res_yvals, leftx, 2)\n",
    "    left_fitx = left_fit[0]*np.array(yvals)*np.array(yvals) + left_fit[1]*np.array(yvals) + left_fit[2]\n",
    "    left_fitx = np.array(left_fitx, np.int32)\n",
    "    right_fit = np.polyfit(res_yvals, rightx, 2)\n",
    "    right_fitx = right_fit[0]*np.array(yvals)*np.array(yvals) + right_fit[1]*np.array(yvals) + right_fit[2]\n",
    "    right_fitx = np.array(right_fitx, np.int32)\n",
    "\n",
    "    # Creating boundary for output lane to be used in cv2.fillPoly() function. \n",
    "    # Left downward edge first and then right upward edge, and concatenate them.\n",
    "    left_lane = \\\n",
    "            np.array(list(zip(np.concatenate((left_fitx - window_width/2, left_fitx[::-1] + window_width/2), axis=0),\n",
    "                                         np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "    right_lane = \\\n",
    "            np.array(list(zip(np.concatenate((right_fitx - window_width/2, right_fitx[::-1] + window_width/2), axis=0),\n",
    "                                         np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "    inner_lane = \\\n",
    "        np.array(list(zip(np.concatenate((left_fitx + window_width / 2, right_fitx[::-1] - window_width / 2), axis=0),\n",
    "                          np.concatenate((yvals, yvals[::-1]), axis=0))), np.int32)\n",
    "\n",
    "    road = np.zeros_like(output)\n",
    "    road_bkg = np.zeros_like(img)\n",
    "    cv2.fillPoly(road, [left_lane], color=[255, 0, 0])\n",
    "    cv2.fillPoly(road, [right_lane], color=[0, 0, 255])\n",
    "    cv2.fillPoly(road, [inner_lane], color=[0, 255, 0])\n",
    "    cv2.fillPoly(road_bkg, [left_lane], color=[255, 255, 255])\n",
    "    cv2.fillPoly(road_bkg, [right_lane], color=[255, 255, 255])\n",
    "    \n",
    "    road_warped = cv2.warpPerspective(road, Minv, img_size, flags=cv2.INTER_LINEAR)\n",
    "    road_warped_bkg = cv2.warpPerspective(road_bkg, Minv, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    base = cv2.addWeighted(undistort, 1, road_warped_bkg, -1.0, 0.0) \n",
    "    output = cv2.addWeighted(base, 1, road_warped, 0.8, 0.0) \n",
    "    \n",
    "    ym_per_pix = curve_centers.ym_per_pix # meters per pixel in y dimension\n",
    "    xm_per_pix = curve_centers.xm_per_pix # meters per pixel in x dimension\n",
    "    \n",
    "    # Caluculate the left and right lane curveture and average them\n",
    "    left_curve_fit_cr = np.polyfit(np.array(res_yvals, np.float32)*ym_per_pix, \n",
    "                                                np.array(leftx, np.float32) * xm_per_pix, 2)\n",
    "    left_curverad =((1 + (2*left_curve_fit_cr[0]*yvals[-1]*ym_per_pix + left_curve_fit_cr[1])**2)**1.5) \\\n",
    "                        / np.absolute(2*left_curve_fit_cr[0])\n",
    "    right_curve_fit_cr = np.polyfit(np.array(res_yvals, np.float32)*ym_per_pix, \n",
    "                                                np.array(rightx, np.float32) * xm_per_pix, 2)\n",
    "    right_curverad =((1 + (2*right_curve_fit_cr[0]*yvals[-1]*ym_per_pix + right_curve_fit_cr[1])**2)**1.5) \\\n",
    "                        / np.absolute(2*right_curve_fit_cr[0])\n",
    "    curverad = (left_curverad +right_curverad) / 2\n",
    "    \n",
    "    left_fitx = left_fit[0]*np.array(yvals)*np.array(yvals) + left_fit[1]*np.array(yvals) + left_fit[2]\n",
    "    left_fitx = np.array(left_fitx, np.int32)\n",
    "  \n",
    "    \n",
    "    # Caluculate the offset of the car on the road\n",
    "    camera_center = (left_fitx[-1] + right_fitx[-1]) / 2\n",
    "    center_diff = (camera_center - warped.shape[1]/2) * xm_per_pix\n",
    "    side_pos = 'left'\n",
    "    if center_diff <= 0:\n",
    "        side_pos = 'right'\n",
    "\n",
    "    # Draw the text showing curvature, offset, and speed\n",
    "    cv2.putText(output, 'Radius of Curvature = ' + str(round(curverad, 3)) + '(m)', (50, 50),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(output, 'Vehicle is ' + str(round(center_diff, 3)) + 'm ' + side_pos +\n",
    "                ' of center', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    # Visualize undistortion\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(undistort)\n",
    "    verts = [src_upper_left, src_upper_right, src_bottom_right, src_bottom_left, src_upper_left]\n",
    "    codes = [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n",
    "    patch = patches.PathPatch(Path(verts, codes), facecolor='none', edgecolor='red', lw=2)\n",
    "    ax1.add_patch(patch)\n",
    "    ax1.set_title('Undistorted Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    ax2.imshow(output, cmap='gray')\n",
    "    ax2.set_title('Final Image (' + fname.split('/')[-1] + ')', fontsize=30)\n",
    "    plt.savefig('output_images/result_test' + str(idx+1) + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
